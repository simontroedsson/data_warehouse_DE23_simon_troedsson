# Exercise 1 - Extracting and loading

In this exercise, you get to familiarize yourself with snowflake roles, extracting and loading to snowflake using dlt.

> [!NOTE]
> These exercises covers lectures 05-08.

## 0. Movie analyst

In the lecture `setup_dlt` we setup a reader and writer role and also an analyst role for the database movies. However we never granted any privileges to that role. It needs the following privileges to do its work:

- read data from tables and views in public schema
- use dev_wh
- create views
- create tables
- create temporary tables

Check that it has the correct grants and future grants. Now assign this role to your user, and do some manual testing to see that it works.

## 1. Which role to use?

Use the correct role and do the following

> [!NOTE]
> Correct role refers to the system defined role and custom role that is most suitable for the task, i.e. don't use a top-level role such as ACCOUNTADMIN to do everything.
> Follow the principle of least privilege (PoLP) - only provide necessary access for roles to perform their duties.

&nbsp; a) Create a marketing virtual warehouse called marketing_wh with size xs, 1 min suspend time, it should autoresume, suspend initially and give it a suitable comment.

&nbsp; b) Now create a database called ifood, and add a staging layer by creating a schema called staging.

&nbsp; c) Create a user called extract_loader and setup its credentials.

&nbsp; d) Create a role marketing_dlt_role and grant it access to staging.

&nbsp; e) Assign marketing_dlt_role to extract_loader user.

## 2. Load csv marketing data to snowflake

Load this [marketing data](https://www.kaggle.com/datasets/fayez7/ifood-marketing-campaigns) into the staging layer using dlt.

## 3. Load olympics data from postgres to snowflake

Download this [csv file of olympics data](https://www.kaggle.com/datasets/nitishsharma01/olympics-124-years-datasettill-2020). Setup a database in postgres and copy in the data from the csv file into postgres. Then load this data using dlt into snowflake.

<!-- MOVE TO project
In lecture `extract_load_api_dlt` we loaded ads from data engineering into snowflake data_field_job_ads table. The data field includes more roles than data engineering, so also find other data related job ads such as data science and data analyst and load them into the same table.  -->

## 4. Load parking API to snowflake (\*\*)

Start by asking for an API key in [open stockholm `Trafikkontorets trafik- och vägdata som öppna data`](https://openstreetgs.stockholm.se/home/). Then go into [parkering - API](https://openstreetgs.stockholm.se/Home/Parking), read the documentation and try to load some data you find interesting into snowflake.

## 5. Theory questions

These study questions are good to get an overview of snowflake roles and dlt for extracting and loading data.

&nbsp; a) Why is the principal of least privilege important in a company?
The principle of least privilige means that you should not have more access than you need to perform your tasks in the company. 

&nbsp; b) Explain the role of dlt in managing data pipelines.
The role of DLT is to extract data from different sources and load it into a target system such as a datawarehouse, datalake or database.

&nbsp; c) What is a data connector and why is it important in data integration?
Data connector is a software component that facilitates data transfer between different system or applications. 
It is important because it enables organizations to seamlessly connect and unify data from different sources

&nbsp; d) What is the difference between data extraction and data loading?
Data extraction is the process of retrieving data from a source system or database.
Data loading is the process of transferring or inserting the extracted (and usually transformed) data into a target system, such as a data warehouse, data lake, database, or other storage systems. 
This is typically the final step in the ETL process.

&nbsp; e) What is ELT and how does it differ from ETL?
ETL (Extract, Transform, Load) and ELT (Extract, Load, Transform) are two different approaches to data integration and processing in data pipelines. 
Both involve moving data from source systems to a target system, but they differ in the order in which the transformation of data occurs.

&nbsp; f) Discuss the advantages of performing data transformations after loading the data.
faster access to data
leverages the scaleable computing power of modern data warehouses making it effiecent to handle large volumes of data

&nbsp; g) What is the purpose of roles in Snowflake?
The purpose of roles is to manage access control. they help administrators to control and restrict access to different resources within a snowflake account.

&nbsp; h) Explain the difference between USAGE and OWNERSHIP privileges.
USAGE is typically used to give users or roles permission to access objects without granting them the ability to alter or manage those objects. 
For example, a user might be granted USAGE on a database or schema so they can query the tables within it, but they wouldn’t be able to create, modify, or drop any objects within that schema.

OWNERSHIP is used to delegate full administrative control over an object to a specific role. 
This privilege allows a role not only to use and modify the object but also to manage its access control and security settings.


&nbsp; i) What information is required to create a user in Snowflake?
username
Identifier for the user; must be unique for your account.
The identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier string is enclosed in double quotes 
(e.g. "My object"). Identifiers enclosed in double quotes are also case-sensitive.

## Glossary

Fill in this table either by copying this into your own markdown file or copy it into a spreadsheet if you feel that is easier to work with.

| terminology         | explanation |
| ------------------- | ----------- |
| SYSADMIN            | The SYSADMIN role is a system-defined role that has privileges to create warehouses, databases, and database objects in an account and grant those privileges to other roles            |
| USERADMIN           | The user administrator (USERADMIN) role includes the privileges to create and manage users and roles (assuming ownership of those roles or users has not been transferred to another role).            |
| ORGADMIN            | The ORGADMIN role is designed to provide administrative privileges at the account level in Snowflake. This means that users assigned the ORGADMIN role have the highest level of authority within the organization. They have full control over all Snowflake resources, including databases, schemas, tables, and views            |
| SECURITYADMIN       | The security administrator (i.e users with the SECURITYADMIN system role) role includes the global MANAGE GRANTS privilege to grant or revoke privileges on objects in the account. The USERADMIN role is a child of this role in the default access control hierarchy.            |
| ACCOUNTADMIN        | The ACCOUNTADMIN role is intended for performing initial setup tasks in the system and managing account-level objects and tasks on a day-to-day basis. As such, it should not be used to create objects in your account, unless you absolutely need these objects to have the highest level of secure access.            |
| role inheritance    | Role inheritance means that you can for example grant role1 to role2 which gives role2 all the privligies of role1. This way you can save alot of time by not having to give specific privligies to a role and instead let it inherit from another role that already have been granted these priviligies            |
| PUBLIC role         | In Snowflake, a "public role" is a default role that comes preconfigured with the Snowflake account. This role is intended to be a very basic role with limited privileges, primarily for users who need minimal access to the system. It can be assigned to users who need to interact with the system but do not require elevated privileges.            |
| public schema       | The public schema in Snowflake is a special default schema that is automatically created in every new database when it is first created. Here’s a breakdown of what the public schema is and its characteristics:            |
| API                 | An API (Application Programming Interface) is a set of rules and protocols that allows different software applications to communicate with each other. APIs define methods and data formats that applications use to interact with each other, either within a system or across different systems.            |
| ETL                 | Stands for extract transform load. is a set of processes for transfering data from a source systems to a target system.            |
| ELT                 | Stands for extract load transform. A process for transfering data from a source to a target, the transformation happens in the datawarehouse            |
| data ingestion      | Data ingestion referes to the process of importing or loading data from source system to a target system.           |
| batch ingestion     | Batch ingestion refers to the process of importing or loading a large amount of data into a system (such as a data warehouse, database, or data lake) at regular intervals or in large chunks, rather than in real-time. This method is commonly used in data integration and ETL (Extract, Transform, Load) processes, where data from various sources is collected, processed, and then ingested into the target system in batches.            |
| streaming ingestion | Streaming ingestion refers to the process of continuously ingesting and processing data in real-time or near-real-time, as it is generated. Unlike batch ingestion, which handles data in large chunks at scheduled intervals, streaming ingestion allows for the immediate capture and processing of data as it arrives, enabling faster insights and more responsive applications.            |
| incremental load    | Incremental load refers to a data loading strategy in which only the data that has changed (i.e., new, updated, or deleted data) since the last load is processed and ingested into a target system, such as a database, data warehouse, or data lake. This approach is efficient because it avoids reprocessing the entire dataset and focuses only on the delta (changes) since the previous load            |
| pagination          | Pagination is a technique used to divide a large set of data into smaller, manageable chunks or pages. This is especially useful when dealing with APIs or databases that return large amounts of data, making it easier to retrieve and display the data in sections rather than loading everything at once.            |
| dlt connectors      | DLT connectors refer to Data Loading and Transformation (DLT) connectors, which are tools or components used in data engineering platforms to facilitate the ingestion, transformation, and loading of data from various sources into a target system. These connectors enable seamless integration with different data sources, allowing users to automate data pipelines for real-time or batch data processing            |
| snowflake user      | A user in Snowflake is an individual account that is created within a Snowflake account to allow access to the data warehouse. Users in Snowflake are assigned specific roles and permissions, which determine what actions they can perform within the system. These actions might include running queries, creating databases, or managing other users.            |
| staging layer       | The staging layer is a critical component in data architecture and ETL (Extract, Transform, Load) processes. It serves as an intermediary storage area where data is initially loaded before it undergoes further processing, transformation, and integration. The staging layer helps manage and prepare data for downstream processes, ensuring that data can be handled efficiently and effectively.            |
| granted to          | a command in snowflake to give priviliges        |
| granted on          | a command in snowflake to give priviliges            |
| granted by          | a command in snowflake to give priviliges            |
| secrets.toml        | a file that is used in the python dlt library to setup connection to a snowflake acccount            |
| RBAC                | RBAC (Role-Based Access Control) is a security model used to manage and restrict user access to resources based on their assigned roles within an organization. This model simplifies administration by associating permissions with roles rather than individual users, and then assigning these roles to users.            |
| CRUD operations     | Operation that is used on a database, e.g. select, create, update, delete            |
| resource dlt        | the def resource(): function is defined to fetch and process data from an external source.            |
| source dlt          |             |
| yield python        | In Python, the yield keyword is used in functions to create generators. Generators are a type of iterable, like lists or tuples, but unlike lists, they don’t store their contents in memory. Instead, they generate values on-the-fly, which is particularly useful for handling large datasets or streams of data without consuming large amounts of memory.            |
|                     |             |
|                     |             |