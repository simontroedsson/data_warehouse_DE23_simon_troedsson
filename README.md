# datawarehoue lifecycle course

I learn datawarehouse here

### Glossary
| terminology       | explanation |
| ----------------- | ----------- |
| downstream        | downstream refers to the components that recieves data or output from an earlier stage in the pipeline. Downstream elements are dependant on data or results produced by upstream components.             |
| upstream          | upstream refers to the components that occur earlier in the data flow. These upstream elements are responsible for producing or providing data that will be used by downstream processes           |
| data warehouse    | A data warehouse is a centralized repository that stores large volumes of structured and semi-structured data from various sources within an organization. It is designed to support querying, reporting, and data analysis, enabling businesses to make data-driven decisions. Data warehouses are optimized for read-heavy workloads, meaning they are well-suited for tasks like running complex queries, generating reports, and performing data analysis |
| cloud computing   | Cloud computing is a technology model that provides on-demand access to computing resources over the internet. These resources include servers, storage, databases, networking, software, analytics, and intelligence, all delivered via the cloud rather than on-premises hardware. Cloud computing enables businesses and individuals to use computing resources without having to invest in or manage physical infrastructure  |
| Modern data stack | The Modern Data Stack (MDS) is an ecosystem of cloud-based tools and technologies that streamline the process of collecting, storing, transforming, analyzing, and utilizing data. Example of characteristics of the MDS is its modular capabilities allowing it to integrate different specialized tools that works best for the organizations needs. It is built on cloud infrastructure which provides scalability, flexibility, and cost efficiency.         |
| idempotent        | Idempotent is a term used in computing, particularly in the context of operations, APIs, and mathematics. An operation is considered idempotent if applying it multiple times produces the same result as applying it once. if f(f(x)) = f(x) then 'f' is idempotent. In databases, an idempotent operation ensures that applying the same operation multiple times (e.g., an update) leads to the same result. For example, updating a user's email address to the same value multiple times doesn't change the database after the first update.           |
| OLAP              | OLAP (Online Analytical Processing) is a category of data processing that enables users to interactively analyze large volumes of data from multiple perspectives, typically for the purpose of business intelligence (BI) and decision-making. OLAP systems are designed to handle complex queries that involve aggregating, slicing, dicing, and drilling down into data to extract insights. OLAP is contrasted with OLTP (Online Transaction Processing), which is focused on managing day-to-day transactional data.            |
| OLTP              | OLTP (Online Transaction Processing) is a category of data processing focused on managing and executing short, simple transactions, typically in real-time, that involve the day-to-day operations of a business. OLTP systems are designed to handle a large number of transactions quickly and efficiently, ensuring data accuracy and consistency while supporting multiple users and processes simultaneously.            |
| virtual warehouse | In the context of Snowflake, a Virtual Warehouse is a cluster of compute resources that provides the computational power required to perform tasks like loading data, running queries, and performing data transformations. Virtual Warehouses are key components in Snowflake's architecture and play a crucial role in separating compute from storage, allowing for flexible and scalable processing.            |
| external stage    | An External Stage in Snowflake is a powerful feature that facilitates the integration of Snowflake with external cloud storage systems like AWS S3, Azure Blob Storage, and Google Cloud Storage. By using external stages, you can efficiently load and unload data, manage large datasets, and ensure secure data transfer between Snowflake and your cloud storage provider. This feature enhances Snowflake's flexibility and scalability, making it an essential tool for modern data engineering and analytics workflows.            |
| data consumer     | A data consumer is an individual, application, or system that utilizes data provided by a data source or data repository for various purposes. In the context of data ecosystems and data management, data consumers play a crucial role in leveraging data to generate insights, drive decisions, or support operational functions.            |
| scaling out       | Scaling out refers to the practice of increasing the capacity of a system by adding more instances of a resource, such as servers, nodes, or clusters, rather than upgrading the existing resources to handle greater loads. This approach is often contrasted with scaling up, which involves adding more power (e.g., CPU, memory) to existing resources.            |
| scaling up        | Scaling up refers to the practice of increasing the capacity of a system by enhancing the power of existing resources, such as adding more CPU, memory, or storage to a single instance or server. This approach is also known as vertical scaling. Scaling up is contrasted with scaling out, which involves adding more instances to distribute the load.            |
| snowflake credit  | In Snowflake, a credit is a unit of measurement used to quantify the consumption of compute resources. Snowflake's billing model is based on the usage of these credits, which are associated with the compute resources utilized for executing queries, data transformations, and other processing tasks            |
| securable object  | n Snowflake, a secureable object is any object or resource within the Snowflake environment to which access control policies and permissions can be applied. These objects are integral to Snowflake’s security model, which is based on a role-based access control (RBAC) system. By defining and managing access to secureable objects, Snowflake administrators can control who can view, modify, or manage specific database objects and resources.            |
| snowflake object  | n Snowflake, an object refers to any entity that is created and managed within the Snowflake environment. Objects are fundamental components in Snowflake’s architecture and include various types of database entities and resources that facilitate data storage, processing, and management. e.g. databases, schemas, views, tables, warehouses.            |
| schema            | A Snowflake schema is a type of database schema used in data warehousing and business intelligence that organizes data into a normalized structure. It is designed to reduce data redundancy and improve data integrity by breaking down data into related tables, forming a structure resembling a snowflake            |
| permanent table   | Permanent tables are stored persistently in the database and remain even after the session ends. They are typically created using the CREATE TABLE statement and are meant for long-term storage of data. Temporary tables are created for temporary storage of data within a session or a specific scope.            |
| transient table   | Transient tables are a hybrid between permanent and temporary tables. They persist until explicitly dropped but do not have the same level of data protection as permanent tables.            |
| temporary table   | Temporary tables are designed for storing transient, non-permanent data that is only needed for a specific session or short-term operation.            |
| time-travel       | Snowflake Time Travel enables accessing historical data (i.e. data that has been changed or deleted) at any point within a defined period. It serves as a powerful tool for performing the following tasks: Restoring data-related objects (tables, schemas, and databases) that might have been accidentally or intentionally deleted. Duplicating and backing up data from key points in the past. Analyzing data usage/manipulation over specified periods of time.            |
| fail-safe         | Separate and distinct from Time Travel, Fail-safe ensures historical data is protected in the event of a system failure or other event (e.g. a security breach). Fail-safe provides a (non-configurable) 7-day period during which historical data may be recoverable by Snowflake. This period starts immediately after the Time Travel retention period ends. Note, however, that a long-running Time Travel query will delay moving any data and objects (tables, schemas, and databases) in the account into Fail-safe, until the query completes           |
| view              | In Snowflake, a view is a database object that represents the result of a stored query. Views are used to simplify complex queries, encapsulate logic, and provide a way to present data in a specific format without directly manipulating the underlying tables. They act as virtual tables and do not store data themselves; instead, they dynamically generate the data when queried based on the underlying base tables.            |
| table             | In Snowflake, a table is a fundamental database object that stores data in a structured format. Tables are the primary means of organizing data within a Snowflake database, and they are used to persistently store rows and columns of data. Each table has a schema that defines its structure and relationships with other tables.            |
| DML               | DML (Data Manipulation Language) refers to a subset of SQL commands used to manipulate and manage data within a database. DML operations are fundamental for interacting with the data stored in tables, allowing users to insert, update, delete, and retrieve data. These operations are essential for maintaining and querying the data according to application requirements            |
| DDL               | DDL (Data Definition Language) commands are essential for defining and managing the structure of database objects in Snowflake and other SQL databases. These commands include CREATE, ALTER, DROP, TRUNCATE, and RENAME, and are used to set up and modify database schemas, tables, views, and other objects. Understanding and effectively using DDL commands helps in designing and maintaining an efficient and well-organized database schema.            |
| DQL               | DQL (Data Query Language) encompasses SQL commands used for querying and retrieving data from databases. The primary DQL command is SELECT, which allows users to fetch and view data based on various criteria. In Snowflake, SELECT queries benefit from advanced features such as support for semi-structured data, query history, result caching, and materialized views. Effective use of DQL commands is crucial for efficient data analysis and reporting.            |
| DCL               | DCL (Data Control Language) commands are essential for managing access to data in a database. The primary DCL commands, GRANT and REVOKE, allow for the assignment and removal of permissions on database objects. In Snowflake, the role-based access control system provides a structured way to manage permissions, ensuring that users have appropriate access based on their roles. Implementing effective DCL practices helps maintain data security, integrity, and compliance.            |

| terminology         | explanation |
| ------------------- | ----------- |
| SYSADMIN            | The SYSADMIN role is a system-defined role that has privileges to create warehouses, databases, and database objects in an account and grant those privileges to other roles            |
| USERADMIN           | The user administrator (USERADMIN) role includes the privileges to create and manage users and roles (assuming ownership of those roles or users has not been transferred to another role).            |
| ORGADMIN            | The ORGADMIN role is designed to provide administrative privileges at the account level in Snowflake. This means that users assigned the ORGADMIN role have the highest level of authority within the organization. They have full control over all Snowflake resources, including databases, schemas, tables, and views            |
| SECURITYADMIN       | The security administrator (i.e users with the SECURITYADMIN system role) role includes the global MANAGE GRANTS privilege to grant or revoke privileges on objects in the account. The USERADMIN role is a child of this role in the default access control hierarchy.            |
| ACCOUNTADMIN        | The ACCOUNTADMIN role is intended for performing initial setup tasks in the system and managing account-level objects and tasks on a day-to-day basis. As such, it should not be used to create objects in your account, unless you absolutely need these objects to have the highest level of secure access.            |
| role inheritance    | Role inheritance means that you can for example grant role1 to role2 which gives role2 all the privligies of role1. This way you can save alot of time by not having to give specific privligies to a role and instead let it inherit from another role that already have been granted these priviligies            |
| PUBLIC role         | In Snowflake, a "public role" is a default role that comes preconfigured with the Snowflake account. This role is intended to be a very basic role with limited privileges, primarily for users who need minimal access to the system. It can be assigned to users who need to interact with the system but do not require elevated privileges.            |
| public schema       | The public schema in Snowflake is a special default schema that is automatically created in every new database when it is first created. Here’s a breakdown of what the public schema is and its characteristics:            |
| API                 | An API (Application Programming Interface) is a set of rules and protocols that allows different software applications to communicate with each other. APIs define methods and data formats that applications use to interact with each other, either within a system or across different systems.            |
| ETL                 | Stands for extract transform load. is a set of processes for transfering data from a source systems to a target system.            |
| ELT                 | Stands for extract load transform. A process for transfering data from a source to a target, the transformation happens in the datawarehouse            |
| data ingestion      | Data ingestion referes to the process of importing or loading data from source system to a target system.           |
| batch ingestion     | Batch ingestion refers to the process of importing or loading a large amount of data into a system (such as a data warehouse, database, or data lake) at regular intervals or in large chunks, rather than in real-time. This method is commonly used in data integration and ETL (Extract, Transform, Load) processes, where data from various sources is collected, processed, and then ingested into the target system in batches.            |
| streaming ingestion | Streaming ingestion refers to the process of continuously ingesting and processing data in real-time or near-real-time, as it is generated. Unlike batch ingestion, which handles data in large chunks at scheduled intervals, streaming ingestion allows for the immediate capture and processing of data as it arrives, enabling faster insights and more responsive applications.            |
| incremental load    | Incremental load refers to a data loading strategy in which only the data that has changed (i.e., new, updated, or deleted data) since the last load is processed and ingested into a target system, such as a database, data warehouse, or data lake. This approach is efficient because it avoids reprocessing the entire dataset and focuses only on the delta (changes) since the previous load            |
| pagination          | Pagination is a technique used to divide a large set of data into smaller, manageable chunks or pages. This is especially useful when dealing with APIs or databases that return large amounts of data, making it easier to retrieve and display the data in sections rather than loading everything at once.            |
| dlt connectors      | DLT connectors refer to Data Loading and Transformation (DLT) connectors, which are tools or components used in data engineering platforms to facilitate the ingestion, transformation, and loading of data from various sources into a target system. These connectors enable seamless integration with different data sources, allowing users to automate data pipelines for real-time or batch data processing            |
| snowflake user      | A user in Snowflake is an individual account that is created within a Snowflake account to allow access to the data warehouse. Users in Snowflake are assigned specific roles and permissions, which determine what actions they can perform within the system. These actions might include running queries, creating databases, or managing other users.            |
| staging layer       | The staging layer is a critical component in data architecture and ETL (Extract, Transform, Load) processes. It serves as an intermediary storage area where data is initially loaded before it undergoes further processing, transformation, and integration. The staging layer helps manage and prepare data for downstream processes, ensuring that data can be handled efficiently and effectively.            |
| granted to          | a command in snowflake to give priviliges        |
| granted on          | a command in snowflake to give priviliges            |
| granted by          | a command in snowflake to give priviliges            |
| secrets.toml        | a file that is used in the python dlt library to setup connection to a snowflake acccount            |
| RBAC                | RBAC (Role-Based Access Control) is a security model used to manage and restrict user access to resources based on their assigned roles within an organization. This model simplifies administration by associating permissions with roles rather than individual users, and then assigning these roles to users.            |
| CRUD operations     | Operation that is used on a database, e.g. select, create, update, delete            |
| resource dlt        | the def resource(): function is defined to fetch and process data from an external source.            |
| source dlt          |             |
| yield python        | In Python, the yield keyword is used in functions to create generators. Generators are a type of iterable, like lists or tuples, but unlike lists, they don’t store their contents in memory. Instead, they generate values on-the-fly, which is particularly useful for handling large datasets or streams of data without consuming large amounts of memory.            |
|                     |             |
|                     |             |